{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_raw = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter= '\\t', header=None)\n",
    "sms_raw.columns = ['review', 'positive_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_raw['review'].astype(str);\n",
    "sms_raw['review'] = sms_raw['review'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = ['good', 'great', 'impressed', 'excellent', 'positive', 'best', 'wonderful', 'perfect', 'love', 'like']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.review.str.contains(\n",
    "        str(key),\n",
    "        case=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sms_raw['positive_review'] = sms_raw['positive_review'].astype(str)\n",
    "sms_raw['positive_review'] = (sms_raw['positive_review'] == '1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = sms_raw[keywords]\n",
    "target = sms_raw['positive_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "bnb = BernoulliNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 301\n"
     ]
    }
   ],
   "source": [
    "#fit the model to the daya\n",
    "bnb.fit(data,target)\n",
    "\n",
    "#classify, store results in new variable\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "#display results\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Model:\n",
      "70%\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy of Model:\")\n",
    "print (\"{0:.0f}%\".format((1000-301)/1000*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[478,  22],\n",
       "       [279, 221]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reviews correctly identified: 478 of 500\n",
      "Positive reviews correctly identified: 221 of 500\n"
     ]
    }
   ],
   "source": [
    "print(\"Negative reviews correctly identified: 478 of 500\")\n",
    "print(\"Positive reviews correctly identified: 221 of 500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confusion matrix shows that we had 22 false positives and 279 false negatives. Thus, our greater problem is identifying positive reviews (which is what our classifier was attempting to acheive). Perhaps we could have more success if we created a classifier based of negative reviews (rather than positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords2 = ['bad', 'hate', 'stupid', 'poor', 'terrible', 'not good', 'negative', 'worst', 'trash', 'junk', 'dont buy', 'don\\'t buy', 'dislike', 'hate', 'waste']\n",
    "\n",
    "for key in keywords2:\n",
    "    sms_raw[str(key)] = sms_raw.review.str.contains(\n",
    "        str(key),\n",
    "        case=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2 = sms_raw[keywords2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 405\n"
     ]
    }
   ],
   "source": [
    "#fit the model to the daya\n",
    "bnb.fit(data2,target)\n",
    "\n",
    "#classify, store results in new variable\n",
    "y_pred = bnb.predict(data2)\n",
    "\n",
    "#display results\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data2.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 96, 404],\n",
       "       [  1, 499]], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is even higher ratio of mislabelled points, perhaps we can improve our originally classifier by including more keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = ['good', 'great', 'impressed', 'excellent', 'positive', 'best', 'wonderful', \n",
    "            'perfect', 'love', ':-\\)', ':\\)', 'like', 'amazing', 'clever', 'delight',\n",
    "           'sweet', 'sweetest', 'endorse', 'enjoy',\n",
    "           'excited', 'happy', 'helpful', 'proud', 'relax', 'safe',\n",
    "           'smooth', 'thrive', 'yippee', 'wonder', 'wonders', 'thrilled']\n",
    "\n",
    "for key in keywords:\n",
    "    sms_raw[str(key)] = sms_raw.review.str.contains(\n",
    "        str(key),\n",
    "        case=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 287\n"
     ]
    }
   ],
   "source": [
    "data = sms_raw[keywords]\n",
    "target = sms_raw['positive_review']\n",
    "\n",
    "#fit the model to the daya\n",
    "bnb.fit(data,target)\n",
    "\n",
    "#classify, store results in new variable\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "#display results\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[474,  26],\n",
       "       [261, 239]], dtype=int64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do any of your classifiers seem to overfit?:\n",
    "Overfitting doesn't seem to be as much of an issue as underfitting.\n",
    "\n",
    "Which seem to perform the best? Why?:\n",
    "So far, searching for positive reviews has a better performance.\n",
    "\n",
    "What features seemed to be most impactful to performance?:\n",
    "Searching for positive reviews."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
